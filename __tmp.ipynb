{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from model import NeuralNetwork\n",
    "\n",
    "df = pd.read_csv(\"data_v2.csv\", sep=',')\n",
    "df = df.query('n == 1')\n",
    "df = df[[\"Method\", \"Beam Type\", \"L/h\", \"k\", \"result\"]]\n",
    "df = df.drop_duplicates(subset=[\"Method\", \"Beam Type\", \"L/h\", \"k\"])\n",
    "\n",
    "# shuffling the dataframe\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "# preprocessing the data\n",
    "method_dict = {\n",
    "    \"CBT\":0,\n",
    "    \"TBT\":1,\n",
    "    \"PSDBT\":2,\n",
    "    \"ESDBT\":3,\n",
    "    \"HSBDT\":4,\n",
    "    \"HSDBT\":4,\n",
    "    \"TSDBT\":5,\n",
    "    \"ASDBT\":6,\n",
    "    \"PESDBT\":7,\n",
    "    \"ISDBT\":8,\n",
    "    \"ICDBT\":9,\n",
    "    \"ITDBT\":10,\n",
    "}\n",
    "\n",
    "beam_type_dict = {\n",
    "    \"S-S FG\":0,\n",
    "    \"C-F FG\":1,\n",
    "    \"C-C FG\":2\n",
    "}\n",
    "\n",
    "l_h_dict = {\n",
    "    5:0,\n",
    "    20:1,\n",
    "    10:2,\n",
    "    100:3,\n",
    "    30:4\n",
    "}\n",
    "\n",
    "\n",
    "# l_h_mean, l_h_std = df[\"L/h\"].mean(), df[\"L/h\"].std()\n",
    "# k_mean, k_std = df[\"k\"].mean(), df[\"k\"].std()\n",
    "# df[\"L/h\"] = (df[\"L/h\"] - l_h_mean) / l_h_std\n",
    "# df[\"k\"] = (df[\"k\"] - k_mean) / k_std\n",
    "df[\"Method\"] = df[\"Method\"].map(method_dict)\n",
    "df[\"Beam Type\"] = df[\"Beam Type\"].map(beam_type_dict)\n",
    "df[\"L/h\"] = df[\"L/h\"].map(l_h_dict)\n",
    "\n",
    "\n",
    "# to tensor\n",
    "input_data = df[[\"Method\", \"Beam Type\", \"L/h\", \"k\"]].to_numpy()\n",
    "target_data = df[\"result\"].to_numpy()\n",
    "\n",
    "input_tensor = torch.tensor(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_one_hot = torch.nn.functional.one_hot(input_tensor[:,0].type(torch.int64), num_classes=len(method_dict.keys()))\n",
    "beam_type_one_hot = torch.nn.functional.one_hot(input_tensor[:,1].type(torch.int64), num_classes=len(beam_type_dict.keys()))\n",
    "l_h_one_hot = torch.nn.functional.one_hot(input_tensor[:,2].type(torch.int64), num_classes=len(l_h_dict.keys()))\n",
    "\n",
    "X = torch.cat((method_one_hot, beam_type_one_hot, l_h_one_hot, input_tensor[:,3].unsqueeze(1)), dim=1).type(torch.float32)\n",
    "y = torch.tensor(target_data).unsqueeze(1).type(torch.float32)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "X_train, X_test, y_train, y_test = X_train.float(), X_test.float(), y_train.float(), y_test.float()\n",
    "\n",
    "train_k_mean, train_k_std = torch.mean(X_train[:, -1]), torch.std(X_train[:, -1])\n",
    "X_train[:, -1] = (X_train[:, -1] - train_k_mean) / train_k_std\n",
    "X_test[:, -1] = (X_test[:, -1] - train_k_mean) / train_k_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = input_tensor.type(torch.float32)\n",
    "# y = torch.tensor(target_data).unsqueeze(1).type(torch.float32)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "# X_train, X_test, y_train, y_test = X_train.float(), X_test.float(), y_train.float(), y_test.float()\n",
    "\n",
    "# train_k_mean, train_k_std = torch.mean(X_train[:, -1]), torch.std(X_train[:, -1])\n",
    "# X_train[:, -1] = (X_train[:, -1] - train_k_mean) / train_k_std\n",
    "# X_test[:, -1] = (X_test[:, -1] - train_k_mean) / train_k_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.953049 using {'module__hidden_sizes': [30, 30]}\n",
      "-0.183553 (1.592180) with: {'module__hidden_sizes': [20, 20]}\n",
      "0.952865 (0.021380) with: {'module__hidden_sizes': [25, 25]}\n",
      "0.953049 (0.020230) with: {'module__hidden_sizes': [30, 30]}\n",
      "-0.321733 (1.789243) with: {'module__hidden_sizes': [20, 20, 20]}\n",
      "0.951387 (0.023950) with: {'module__hidden_sizes': [25, 25, 25]}\n",
      "-1.395386 (1.676329) with: {'module__hidden_sizes': [30, 30, 30]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch import helper\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.utils import parallel_backend\n",
    "\n",
    "# with parallel_backend('multiprocessing'):\n",
    "model = NeuralNetRegressor(\n",
    "    NeuralNetwork,\n",
    "    criterion=nn.MSELoss,\n",
    "    max_epochs=300,\n",
    "    batch_size=128,\n",
    "    verbose=False,\n",
    "    optimizer=optim.SGD,\n",
    "    train_split=helper.predefined_split(torch.utils.data.TensorDataset(X_test, y_test))\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    ##### for optimizer\n",
    "    # 'optimizer__lr': [0.01],\n",
    "    # 'optimizer__momentum': [0.9],\n",
    "\n",
    "    ##### for module\n",
    "    'module__hidden_sizes':[[20, 20], [25, 25], [30, 30], [20, 20, 20], [25, 25, 25], [30, 30, 30]],\n",
    "    # 'module__activation': [nn.ReLU, nn.Tanh, nn.Sigmoid],\n",
    "    # 'module__dropout_rate': [0.1]\n",
    "}\n",
    "\n",
    "# Create your model, criterion, and grid search object\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "\n",
    "# Fit the grid search to find the best configuration\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "\n",
    "# Best: 0.440983 using {'module__activation': <class 'torch.nn.modules.activation.Sigmoid'>, 'module__dropout_rate': 0.1, 'module__hidden_size': 30, 'module__hidden_size2': 25, 'optimizer__lr': 0.01, 'optimizer__momentum': 0.9}\n",
    "# Best: 0.498547 using {'module__activation': <class 'torch.nn.modules.activation.Sigmoid'>, 'module__dropout_rate': 0.1, 'module__hidden_size': 40, 'module__weight_constraint': 3.0, 'optimizer__lr': 0.01, 'optimizer__momentum': 0.9}\n",
    "# Best: 0.622935 using {'module__activation': <class 'torch.nn.modules.activation.Tanh'>, 'module__dropout_rate': 0.1, 'module__hidden_size': 20, 'module__weight_constraint': 3.0, 'optimizer__lr': 0.01, 'optimizer__momentum': 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - Loss 0.4828733205795288 - Test Mean MSE: 11.986270067168444\n",
      "Epoch: 10 - Loss 0.019470922648906708 - Test Mean MSE: 0.9425389359637004\n",
      "Epoch: 20 - Loss 0.005562742240726948 - Test Mean MSE: 0.31747904056456033\n",
      "Epoch: 30 - Loss 0.004124201834201813 - Test Mean MSE: 0.18487190618747618\n",
      "Epoch: 40 - Loss 0.0034989570267498493 - Test Mean MSE: 0.1488740967541206\n",
      "Epoch: 50 - Loss 0.0034203112591058016 - Test Mean MSE: 0.11997310126700052\n",
      "Epoch: 60 - Loss 0.0025313568767160177 - Test Mean MSE: 0.1081151148168052\n",
      "Epoch: 70 - Loss 0.0029837931506335735 - Test Mean MSE: 0.26275413792307784\n",
      "Epoch: 80 - Loss 0.0025142852682620287 - Test Mean MSE: 0.13302999589501358\n",
      "Epoch: 90 - Loss 0.002039284911006689 - Test Mean MSE: 0.1391164500538896\n",
      "Epoch: 100 - Loss 0.0019323122687637806 - Test Mean MSE: 0.08021355838310427\n",
      "Epoch: 110 - Loss 0.001562196179293096 - Test Mean MSE: 0.06336010374673982\n",
      "Epoch: 120 - Loss 0.0013727023033425212 - Test Mean MSE: 0.10829485916509861\n",
      "Epoch: 130 - Loss 0.001182345673441887 - Test Mean MSE: 0.06342626199489687\n",
      "Epoch: 140 - Loss 0.0009302838589064777 - Test Mean MSE: 0.05810217159550365\n",
      "Epoch: 150 - Loss 0.0009666226687841117 - Test Mean MSE: 0.05388129629739901\n",
      "Epoch: 160 - Loss 0.0009704908588901162 - Test Mean MSE: 0.1304424565012862\n",
      "Epoch: 170 - Loss 0.0006741501856595278 - Test Mean MSE: 0.06498415877179402\n",
      "Epoch: 180 - Loss 0.000551697623450309 - Test Mean MSE: 0.08569124268322456\n",
      "Epoch: 190 - Loss 0.0007617057999596 - Test Mean MSE: 0.17351523841299663\n"
     ]
    }
   ],
   "source": [
    "def test(model, test_dataloader):\n",
    "    total_samples = 0\n",
    "    total_mse = 0.0  # Initialize MSE to 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            batch_size = labels.size(0)  # Get the batch size\n",
    "\n",
    "            # Calculate the Mean Squared Error (MSE) for this batch\n",
    "            mse = ((outputs - labels).pow(2).sum()).item()\n",
    "            total_mse += mse\n",
    "            total_samples += batch_size\n",
    "\n",
    "    # Calculate the average MSE over the entire dataset\n",
    "    mean_mse = total_mse / total_samples\n",
    "\n",
    "    return mean_mse\n",
    "\n",
    "\n",
    "# Create a TensorDataset\n",
    "training_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 64\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "model = NeuralNetwork(hidden_sizes=[25, 25], activation=nn.ReLU, dropout_rate=0.1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(300):\n",
    "    cur_loss = 0\n",
    "    total_len = 0\n",
    "    for data, target in training_dataloader:  # Iterate through your data\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.float(), target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cur_loss += loss\n",
    "        total_len += data.shape[0]\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        test_mse = test(model, test_dataloader)\n",
    "        print(f\"Epoch: {epoch} - Loss {cur_loss/total_len} - Test Mean MSE: {test_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3973000049591064 1.4225157499313354\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        print(labels[4].item(), outputs[4].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.7491], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_one_hot = torch.nn.functional.one_hot(torch.tensor(4), num_classes=len(method_dict.keys()))\n",
    "beam_type_one_hot = torch.nn.functional.one_hot(torch.tensor(0), num_classes=len(beam_type_dict.keys()))\n",
    "l_h_one_hot = torch.nn.functional.one_hot(torch.tensor(1), num_classes=len(l_h_dict.keys()))\n",
    "k = (0.5 - train_k_mean) / train_k_std\n",
    "\n",
    "test_tensor = torch.cat((method_one_hot, beam_type_one_hot, l_h_one_hot, torch.tensor([k])), dim=0).type(torch.float32)\n",
    "\n",
    "output = model(test_tensor)\n",
    "output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
